{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "import random\n",
    "import re\n",
    "import bs4\n",
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras.preprocessing.sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_file, test_file, extra_file):\n",
    "    train_set = pd.read_csv(train_file, header=0, sep='\\t')\n",
    "    test_set = pd.read_csv(test_file, header=0, sep='\\t')\n",
    "    extra_set = pd.read_csv(extra_file, header=0, sep='\\t', error_bad_lines=False, warn_bad_lines=True)\n",
    "    print('Train set info:')\n",
    "    train_set.info()\n",
    "    print('Test set info:')\n",
    "    test_set.info()\n",
    "    print('Extra set info:')\n",
    "    extra_set.info()\n",
    "\n",
    "    return train_set, test_set, extra_set\n",
    "\n",
    "\n",
    "def raw_to_words(df, column, remove_stopwords=False):\n",
    "    wordnet = nltk.stem.WordNetLemmatizer()\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "    df[column] = df[column].map(lambda x: bs4.BeautifulSoup(x, 'html.parser').get_text())\n",
    "    df[column] = df[column].map(lambda x: re.sub(r'[^a-zA-Z,!?\\'\\`]', ' ', x))\n",
    "    df[column] = df[column].map(lambda x: x.lower().split())\n",
    "    df[column] = df[column].map(lambda x: [wordnet.lemmatize(y) for y in x])\n",
    "    if remove_stopwords:\n",
    "        df[column] = df[column].map(lambda x: [y for y in x if y not in stopwords])\n",
    "\n",
    "    df['num_words'] = df[column].apply(lambda x: len(x))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def raw_to_texts(df, column, remove_stopwords=False):\n",
    "    df = raw_to_words(df, column, remove_stopwords)\n",
    "    print(df.describe())\n",
    "    df[column] = df[column].map(lambda x: ' '.join(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 43043: expected 2 fields, saw 3\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 25000 entries, 0 to 24999\nData columns (total 3 columns):\nid           25000 non-null object\nsentiment    25000 non-null int64\nreview       25000 non-null object\ndtypes: int64(1), object(2)\nmemory usage: 586.0+ KB\nTest set info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 25000 entries, 0 to 24999\nData columns (total 2 columns):\nid        25000 non-null object\nreview    25000 non-null object\ndtypes: object(2)\nmemory usage: 390.7+ KB\nExtra set info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 49998 entries, 0 to 49997\nData columns (total 2 columns):\nid        49998 non-null object\nreview    49998 non-null object\ndtypes: object(2)\nmemory usage: 781.3+ KB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         sentiment     num_words\ncount  25000.00000  25000.000000\nmean       0.50000    127.947480\nstd        0.50001     96.277805\nmin        0.00000      5.000000\n25%        0.00000     68.000000\n50%        0.50000     95.000000\n75%        1.00000    156.000000\nmax        1.00000   1463.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          num_words\ncount  25000.000000\nmean     124.983120\nstd       93.685699\nmin        3.000000\n25%       68.000000\n50%       93.000000\n75%      152.000000\nmax     1248.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          num_words\ncount  49998.000000\nmean     128.484659\nstd       96.426765\nmin        3.000000\n25%       69.000000\n50%       95.000000\n75%      157.000000\nmax     1455.000000\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set, extra_set = load_data(r'E:\\OpenSourceDatasetCode\\Dataset\\Bag of Words Meets Bags of Popcorn\\labeledTrainData.tsv',\n",
    "                                           r'E:\\OpenSourceDatasetCode\\Dataset\\Bag of Words Meets Bags of Popcorn\\testData.tsv',\n",
    "                                           r'E:\\OpenSourceDatasetCode\\Dataset\\Bag of Words Meets Bags of Popcorn\\unlabeledTrainData.tsv')\n",
    "\n",
    "train_df = raw_to_texts(train_set, 'review', remove_stopwords=True)\n",
    "test_df = raw_to_texts(test_set, 'review', remove_stopwords=True)\n",
    "extra_df = raw_to_texts(extra_set, 'review', remove_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y = train_df.sentiment\n",
    "del train_df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews = pd.concat([train_df, test_df, extra_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import text\n",
    "sequence_tokenizer = text.Tokenizer()\n",
    "sequence_tokenizer.fit_on_texts(line for line in all_reviews['review'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_len = len(sequence_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168114"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "def texts_to_sequences(df, column, tokenizer, maxlen=300):\n",
    "    seq = tokenizer.texts_to_sequences(line for line in df[column].values)\n",
    "    print(type(seq[0]))\n",
    "    print('mean:', np.mean([len(x) for x in seq]))\n",
    "    print('std:', np.std([len(x) for x in seq]))\n",
    "    print('median:', np.median([len(x) for x in seq]))\n",
    "    print('max:', np.max([len(x) for x in seq]))\n",
    "    seq = sequence.pad_sequences(seq, maxlen=maxlen, padding='post', truncating='post')\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\nmean: 127.5788\nstd: 95.8301327901\nmedian: 95.0\nmax: 1462\n"
     ]
    }
   ],
   "source": [
    "train_X = texts_to_sequences(train_df, 'review', sequence_tokenizer, maxlen=1500)\n",
    "del train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
