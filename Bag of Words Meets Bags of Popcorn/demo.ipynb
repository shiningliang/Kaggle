{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 载入数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r'E:\\OpenSourceDatasetCode\\Dataset\\Bag of Words Meets Bags of Popcorn\\labeledTrainData.tsv', header=0, delimiter='\\t', quoting=3)\n",
    "test = pd.read_csv(r'E:\\OpenSourceDatasetCode\\Dataset\\Bag of Words Meets Bags of Popcorn\\testData.tsv', header=0, delimiter='\\t', quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 25000 entries, 0 to 24999\nData columns (total 3 columns):\nid           25000 non-null object\nsentiment    25000 non-null int64\nreview       25000 non-null object\ndtypes: int64(1), object(2)\nmemory usage: 586.0+ KB\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 25000 entries, 0 to 24999\nData columns (total 2 columns):\nid        25000 non-null object\nreview    25000 non-null object\ndtypes: object(2)\nmemory usage: 390.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulid_train_test_data(train, test, train_ratio = 0.8):\n",
    "    '''\n",
    "    把IMDB的评论转成词序列\n",
    "    '''\n",
    "    reviews = []\n",
    "    vocab = defaultdict(float)\n",
    "    for i in range(train.shape[0]):\n",
    "        label = train['sentiment'][i]\n",
    "        # 去掉HTML标签，拿到内容\n",
    "        review_text = BeautifulSoup(train['review'][i], \"html.parser\").get_text()\n",
    "        # 用正则表达式取出符合规范的部分\n",
    "        review_text = clean_str(review_text.strip())\n",
    "        # 小写化所有的词，并转成词list\n",
    "        review_text = review_text.lower().split()\n",
    "        # 去除停用词\n",
    "        stops = set(stopwords.words('english'))\n",
    "        review_text = [w for w in review_text if not w in stops]\n",
    "        words = set(review_text)\n",
    "        for word in words:\n",
    "            vocab[word] += 1\n",
    "        sample = {'label': label,\n",
    "                  'review_text': review_text,\n",
    "                  'num_words': len(review_text),\n",
    "                  'split': int(np.random.rand() < train_ratio)}\n",
    "        reviews.append(sample)\n",
    "        \n",
    "    for i in range(test.shape[0]):\n",
    "        # 去掉HTML标签，拿到内容\n",
    "        review_text = BeautifulSoup(test['review'][i], \"html.parser\").get_text()\n",
    "        # 用正则表达式取出符合规范的部分\n",
    "        review_text = clean_str(review_text.strip())\n",
    "        # 小写化所有的词，并转成词list\n",
    "        review_text = review_text.lower().split()\n",
    "        # 去除停用词\n",
    "        stops = set(stopwords.words('english'))\n",
    "        review_text = [w for w in review_text if not w in stops]\n",
    "        words = set(review_text)\n",
    "        for word in words:\n",
    "            vocab[word] += 1\n",
    "        sample = {'label': -1,\n",
    "                  'review_text': review_text,\n",
    "                  'num_words': len(review_text),\n",
    "                  'split': -1}\n",
    "        reviews.append(sample)\n",
    "    # 返回words\n",
    "    return reviews, vocab\n",
    "\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" \\( \", string) \n",
    "    string = re.sub(r\"\\)\", \" \\) \", string) \n",
    "    string = re.sub(r\"\\?\", \" \\? \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)    \n",
    "    return string.strip().lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded!\nnumber of sentences: 50000\nvocab size: 114639\nmax sentence length: 1571\n"
     ]
    }
   ],
   "source": [
    "reviews, vocab = bulid_train_test_data(train, test)\n",
    "max_len = np.max(pd.DataFrame(reviews)['num_words'])\n",
    "print('data loaded!')\n",
    "print('number of sentences: ' + str(len(reviews)))\n",
    "print('vocab size: ' + str(len(vocab)))\n",
    "print('max sentence length: ' + str(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_matrix(fname, vocab, k=300):\n",
    "    \"\"\"\n",
    "    Loads 300x1 word vecs from Google (Mikolov) word2vec\n",
    "    \"\"\"\n",
    "    word_idx_map = dict()\n",
    "    model = gensim.models.Word2Vec.load_word2vec_format(fname, binary=True)\n",
    "    print('word2vec loaded!')\n",
    "    W = np.random.uniform(-0.25, 0.25, (len(vocab)+1, k))\n",
    "    W[0] = np.zeros(k, dtype=np.float32)\n",
    "    i = 1\n",
    "    j = 0\n",
    "    for word in vocab:\n",
    "        if word in model.vocab:\n",
    "            W[i] = model[word]\n",
    "            j += 1\n",
    "        \n",
    "        word_idx_map[word] = i\n",
    "        i += 1\n",
    "    \n",
    "    del model\n",
    "    return W, word_idx_map, j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_file = r'E:\\ToolsData\\Weights\\GoogleNews-vectors-negative300.bin'\n",
    "W, word_idx_map, num_in_model = build_embedding_matrix(w2v_file, vocab)\n",
    "print('num words already in word2vec: ' + str(num_in_model))\n",
    "with open('imdb_train_val_test.pkl', 'wb') as file:\n",
    "    pickle.dump([reviews, W, word_idx_map, vocab], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
