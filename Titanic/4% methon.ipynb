{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分分钟杀入Kaggle TOP5% 系列（2）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('/Users/zhouzhirui/Documents/evilpsycho/zhihu/kaggle/titanic/data/')\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "combine_df = pd.concat([train_df,test_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine_df['Name_Len'] = combine_df['Name'].apply(lambda x: len(x))\n",
    "combine_df['Name_Len'] = pd.qcut(combine_df['Name_Len'],5)\n",
    "\n",
    "\n",
    "combine_df['Title'] = combine_df['Name'].apply(lambda x: x.split(', ')[1]).apply(lambda x: x.split('.')[0])\n",
    "combine_df['Title'] = combine_df['Title'].replace(['Don','Dona', 'Major', 'Capt', 'Jonkheer', 'Rev', 'Col','Sir','Dr'],'Mr')\n",
    "combine_df['Title'] = combine_df['Title'].replace(['Mlle','Ms'], 'Miss')\n",
    "combine_df['Title'] = combine_df['Title'].replace(['the Countess','Mme','Lady','Dr'], 'Mrs')\n",
    "df = pd.get_dummies(combine_df['Title'],prefix='Title')\n",
    "combine_df = pd.concat([combine_df,df],axis=1)\n",
    "\n",
    "combine_df['Surname'] = combine_df['Name'].apply(lambda x:x.split(',')[0])\n",
    "dead_female_surname = list(set(combine_df[(combine_df.Sex=='female') & (combine_df.Age>=12)\n",
    "                              & (combine_df.Survived==0) & ((combine_df.Parch>0) | (combine_df.SibSp > 0))]['Surname'].values))\n",
    "survive_male_surname = list(set(combine_df[(combine_df.Sex=='male') & (combine_df.Age>=12)\n",
    "                              & (combine_df.Survived==1) & ((combine_df.Parch>0) | (combine_df.SibSp > 0))]['Surname'].values))\n",
    "combine_df['Dead_female_family'] = np.where(combine_df['Surname'].isin(dead_female_surname),0,1)\n",
    "combine_df['Survive_male_family'] = np.where(combine_df['Surname'].isin(survive_male_surname),0,1)\n",
    "combine_df = combine_df.drop(['Name','Surname'],axis=1)\n",
    "\n",
    "group = combine_df.groupby(['Title', 'Pclass'])['Age']\n",
    "combine_df['Age'] = group.transform(lambda x: x.fillna(x.median()))\n",
    "combine_df = combine_df.drop('Title',axis=1)\n",
    "\n",
    "combine_df['IsChild'] = np.where(combine_df['Age']<=12,1,0)\n",
    "combine_df['Age'] = pd.cut(combine_df['Age'],5)\n",
    "combine_df = combine_df.drop('Age',axis=1)\n",
    "combine_df['Familysize'] = combine_df.SibSp + combine_df.Parch\n",
    "combine_df['Familysize'] = np.where(combine_df['Familysize']==0, 'solo',\n",
    "                                    np.where(combine_df['Familysize']<=3, 'normal', 'big'))\n",
    "df = pd.get_dummies(combine_df['Familysize'],prefix='Familysize')\n",
    "combine_df = pd.concat([combine_df,df],axis=1).drop(['SibSp','Parch','Familysize'])\n",
    "\n",
    "combine_df['Ticket_Lett'] = combine_df['Ticket'].apply(lambda x: str(x)[0])\n",
    "combine_df['Ticket_Lett'] = combine_df['Ticket_Lett'].apply(lambda x: str(x))\n",
    "\n",
    "combine_df['High_Survival_Ticket'] = np.where(combine_df['Ticket_Lett'].isin(['1', '2', 'P']),1,0)\n",
    "combine_df['Low_Survival_Ticket'] = np.where(combine_df['Ticket_Lett'].isin(['A','W','3','7']),1,0)\n",
    "combine_df = combine_df.drop(['Ticket','Ticket_Lett'],axis=1)\n",
    "\n",
    "combine_df.Embarked = combine_df.Embarked.fillna('S')\n",
    "df = pd.get_dummies(combine_df['Embarked'],prefix='Embarked')\n",
    "combine_df = pd.concat([combine_df,df],axis=1).drop('Embarked',axis=1)\n",
    "\n",
    "combine_df['Cabin_isNull'] = np.where(combine_df['Cabin'].isnull(),0,1)\n",
    "combine_df = combine_df.drop('Cabin',axis=1)\n",
    "\n",
    "df = pd.get_dummies(combine_df['Pclass'],prefix='Pclass')\n",
    "combine_df = pd.concat([combine_df,df],axis=1).drop('Pclass',axis=1)\n",
    "\n",
    "df = pd.get_dummies(combine_df['Sex'],prefix='Sex')\n",
    "combine_df = pd.concat([combine_df,df],axis=1).drop('Sex',axis=1)\n",
    "\n",
    "\n",
    "combine_df['Fare'] = pd.qcut(combine_df.Fare,3)\n",
    "df = pd.get_dummies(combine_df.Fare,prefix='Fare').drop('Fare_(-0.001, 8.662]',axis=1)\n",
    "combine_df = pd.concat([combine_df,df],axis=1).drop('Fare',axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Title\n",
    "combine_df['Title'] = combine_df['Name'].apply(lambda x: x.split(', ')[1]).apply(lambda x: x.split('.')[0])\n",
    "combine_df['Title'] = combine_df['Title'].replace(['Don','Dona', 'Major', 'Capt', 'Jonkheer', 'Rev', 'Col','Sir','Dr'],'Mr')\n",
    "combine_df['Title'] = combine_df['Title'].replace(['Mlle','Ms'], 'Miss')\n",
    "combine_df['Title'] = combine_df['Title'].replace(['the Countess','Mme','Lady','Dr'], 'Mrs')\n",
    "df = pd.get_dummies(combine_df['Title'],prefix='Title')\n",
    "combine_df = pd.concat([combine_df,df],axis=1)\n",
    "\n",
    "#Name_length\n",
    "combine_df['Name_Len'] = combine_df['Name'].apply(lambda x: len(x))\n",
    "combine_df['Name_Len'] = pd.qcut(combine_df['Name_Len'],5)\n",
    "\n",
    "\n",
    "#Dead_female_family & Survive_male_family\n",
    "combine_df['Surname'] = combine_df['Name'].apply(lambda x:x.split(',')[0])\n",
    "dead_female_surname = list(set(combine_df[(combine_df.Sex=='female') & (combine_df.Age>=12)\n",
    "                              & (combine_df.Survived==0) & ((combine_df.Parch>0) | (combine_df.SibSp > 0))]['Surname'].values))\n",
    "survive_male_surname = list(set(combine_df[(combine_df.Sex=='male') & (combine_df.Age>=12)\n",
    "                              & (combine_df.Survived==1) & ((combine_df.Parch>0) | (combine_df.SibSp > 0))]['Surname'].values))\n",
    "combine_df['Dead_female_family'] = np.where(combine_df['Surname'].isin(dead_female_surname),0,1)\n",
    "combine_df['Survive_male_family'] = np.where(combine_df['Surname'].isin(survive_male_surname),0,1)\n",
    "combine_df = combine_df.drop(['Name','Surname'],axis=1)\n",
    "\n",
    "\n",
    "#Age & isChild\n",
    "group = combine_df.groupby(['Title', 'Pclass'])['Age']\n",
    "combine_df['Age'] = group.transform(lambda x: x.fillna(x.median()))\n",
    "combine_df = combine_df.drop('Title',axis=1)\n",
    "combine_df['IsChild'] = np.where(combine_df['Age']<=12,1,0)\n",
    "combine_df['Age'] = pd.cut(combine_df['Age'],5)\n",
    "combine_df = combine_df.drop('Age',axis=1)\n",
    "\n",
    "#ticket\n",
    "combine_df['Ticket_Lett'] = combine_df['Ticket'].apply(lambda x: str(x)[0])\n",
    "combine_df['Ticket_Lett'] = combine_df['Ticket_Lett'].apply(lambda x: str(x))\n",
    "\n",
    "combine_df['High_Survival_Ticket'] = np.where(combine_df['Ticket_Lett'].isin(['1', '2', 'P']),1,0)\n",
    "combine_df['Low_Survival_Ticket'] = np.where(combine_df['Ticket_Lett'].isin(['A','W','3','7']),1,0)\n",
    "combine_df = combine_df.drop(['Ticket','Ticket_Lett'],axis=1)\n",
    "\n",
    "#Embarked\n",
    "#combine_df = combine_df.drop('Embarked',axis=1)\n",
    "combine_df.Embarked = combine_df.Embarked.fillna('S')\n",
    "df = pd.get_dummies(combine_df['Embarked'],prefix='Embarked')\n",
    "combine_df = pd.concat([combine_df,df],axis=1).drop('Embarked',axis=1)\n",
    "\n",
    "#FamilySize\n",
    "combine_df['FamilySize'] = np.where(combine_df['SibSp']+combine_df['Parch']==0, 'Alone',\n",
    "                                    np.where(combine_df['SibSp']+combine_df['Parch']<=3, 'Small', 'Big'))\n",
    "df = pd.get_dummies(combine_df['FamilySize'],prefix='FamilySize')\n",
    "combine_df = pd.concat([combine_df,df],axis=1).drop(['SibSp','Parch','FamilySize'],axis=1)\n",
    "\n",
    "\n",
    "#Cabin\n",
    "combine_df['Cabin_isNull'] = np.where(combine_df['Cabin'].isnull(),0,1)\n",
    "combine_df = combine_df.drop('Cabin',axis=1)\n",
    "\n",
    "#PClass\n",
    "df = pd.get_dummies(combine_df['Pclass'],prefix='Pclass')\n",
    "combine_df = pd.concat([combine_df,df],axis=1).drop('Pclass',axis=1)\n",
    "\n",
    "\n",
    "#Sex\n",
    "df = pd.get_dummies(combine_df['Sex'],prefix='Sex')\n",
    "combine_df = pd.concat([combine_df,df],axis=1).drop('Sex',axis=1)\n",
    "\n",
    "#Fare\n",
    "combine_df['Fare'].fillna(combine_df['Fare'].dropna().median(),inplace=True)\n",
    "combine_df['Low_Fare'] = np.where(combine_df['Fare']<=8.662,1,0)\n",
    "combine_df['High_Fare'] = np.where(combine_df['Fare']>=26,1,0)\n",
    "combine_df = combine_df.drop('Fare',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = combine_df.drop([\"PassengerId\",\"Survived\"], axis=1).columns\n",
    "le = LabelEncoder()\n",
    "for feature in features:\n",
    "    le = le.fit(combine_df[feature])\n",
    "    combine_df[feature] = le.transform(combine_df[feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Title\n",
    "combine_df['Title'] = combine_df['Name'].apply(lambda x: x.split(', ')[1]).apply(lambda x: x.split('.')[0])\n",
    "combine_df['Title'] = combine_df['Title'].replace(['Don','Dona', 'Major', 'Capt', 'Jonkheer', 'Rev', 'Col','Sir','Dr'],'Mr')\n",
    "combine_df['Title'] = combine_df['Title'].replace(['Mlle','Ms'], 'Miss')\n",
    "combine_df['Title'] = combine_df['Title'].replace(['the Countess','Mme','Lady','Dr'], 'Mrs')\n",
    "df = pd.get_dummies(combine_df['Title'],prefix='Title')\n",
    "combine_df = pd.concat([combine_df,df],axis=1)\n",
    "\n",
    "#Name_length\n",
    "combine_df['Name_Len'] = combine_df['Name'].apply(lambda x: len(x))\n",
    "combine_df['Name_Len'] = pd.qcut(combine_df['Name_Len'],5)\n",
    "\n",
    "#Dead_female_family & Survive_male_family\n",
    "combine_df['Surname'] = combine_df['Name'].apply(lambda x:x.split(',')[0])\n",
    "dead_female_surname = list(set(combine_df[(combine_df.Sex=='female') & (combine_df.Age>=12)\n",
    "                              & (combine_df.Survived==0) & ((combine_df.Parch>0) | (combine_df.SibSp > 0))]['Surname'].values))\n",
    "survive_male_surname = list(set(combine_df[(combine_df.Sex=='male') & (combine_df.Age>=12)\n",
    "                              & (combine_df.Survived==1) & ((combine_df.Parch>0) | (combine_df.SibSp > 0))]['Surname'].values))\n",
    "combine_df['Dead_female_family'] = np.where(combine_df['Surname'].isin(dead_female_surname),0,1)\n",
    "combine_df['Survive_male_family'] = np.where(combine_df['Surname'].isin(survive_male_surname),0,1)\n",
    "combine_df = combine_df.drop(['Name','Surname'],axis=1)\n",
    "\n",
    "#Age & isChild\n",
    "group = combine_df.groupby(['Title', 'Pclass'])['Age']\n",
    "combine_df['Age'] = group.transform(lambda x: x.fillna(x.median()))\n",
    "combine_df = combine_df.drop('Title',axis=1)\n",
    "\n",
    "combine_df['IsChild'] = np.where(combine_df['Age']<=12,1,0)\n",
    "combine_df['Age'] = pd.cut(combine_df['Age'],5)\n",
    "combine_df = combine_df.drop('Age',axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#FamilySize\n",
    "combine_df['FamilySize'] = np.where(combine_df['SibSp']+combine_df['Parch']==0, 'Alone',\n",
    "                                    np.where(combine_df['SibSp']+combine_df['Parch']<=3, 'Small', 'Big'))\n",
    "df = pd.get_dummies(combine_df['FamilySize'],prefix='FamilySize')\n",
    "combine_df = pd.concat([combine_df,df],axis=1).drop(['SibSp','Parch','FamilySize'],axis=1)\n",
    "\n",
    "#Ticket\n",
    "combine_df['Ticket_Lett'] = combine_df['Ticket'].apply(lambda x: str(x)[0])\n",
    "combine_df['Ticket_Lett'] = combine_df['Ticket_Lett'].apply(lambda x: str(x))\n",
    "\n",
    "combine_df['High_Survival_Ticket'] = np.where(combine_df['Ticket_Lett'].isin(['1', '2', 'P']),1,0)\n",
    "combine_df['Low_Survival_Ticket'] = np.where(combine_df['Ticket_Lett'].isin(['A','W','3','7']),1,0)\n",
    "combine_df = combine_df.drop(['Ticket','Ticket_Lett'],axis=1)\n",
    "\n",
    "#Embarked\n",
    "#combine_df = combine_df.drop('Embarked',axis=1)\n",
    "\n",
    "combine_df.Embarked = combine_df.Embarked.fillna('S')\n",
    "df = pd.get_dummies(combine_df['Embarked'],prefix='Embarked')\n",
    "combine_df = pd.concat([combine_df,df],axis=1).drop('Embarked',axis=1)\n",
    "\n",
    "\n",
    "#Cabin\n",
    "combine_df['Cabin_isNull'] = np.where(combine_df['Cabin'].isnull(),0,1)\n",
    "combine_df = combine_df.drop('Cabin',axis=1)\n",
    "\n",
    "#PClass\n",
    "df = pd.get_dummies(combine_df['Pclass'],prefix='Pclass')\n",
    "combine_df = pd.concat([combine_df,df],axis=1).drop('Pclass',axis=1)\n",
    "\n",
    "#Sex\n",
    "df = pd.get_dummies(combine_df['Sex'],prefix='Sex')\n",
    "combine_df = pd.concat([combine_df,df],axis=1).drop('Sex',axis=1)\n",
    "\n",
    "#Fare\n",
    "combine_df['Fare'].fillna(combine_df['Fare'].dropna().median(),inplace=True)\n",
    "combine_df['Low_Fare'] = np.where(combine_df['Fare']<=8.662,1,0)\n",
    "combine_df['High_Fare'] = np.where(combine_df['Fare']>=26,1,0)\n",
    "combine_df = combine_df.drop('Fare',axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_all = combine_df.iloc[:891,:].drop([\"PassengerId\",\"Survived\"], axis=1)\n",
    "Y_all = combine_df.iloc[:891,:][\"Survived\"]\n",
    "X_test = combine_df.iloc[891:,:].drop([\"PassengerId\",\"Survived\"], axis=1)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "svc = SVC()\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "random_forest = RandomForestClassifier(n_estimators=300,min_samples_leaf=4,class_weight={0:0.745,1:0.255})\n",
    "gbdt = GradientBoostingClassifier(n_estimators=500,learning_rate=0.03,max_depth=3)\n",
    "xgb = XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.03)\n",
    "clfs = [logreg, svc, knn, decision_tree, random_forest, gbdt, xgb]\n",
    "\n",
    "kfold = 10\n",
    "cv_results = []\n",
    "for classifier in clfs :\n",
    "    cv_results.append(cross_val_score(classifier, X_all.values, y = Y_all.values, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n",
    "\n",
    "cv_means = []\n",
    "cv_std = []\n",
    "for cv_result in cv_results:\n",
    "    cv_means.append(cv_result.mean())\n",
    "    cv_std.append(cv_result.std())\n",
    "\n",
    "ag = [\"LR\",\"SVC\",'KNN','decision_tree',\"random_forest\",\"GBDT\",\"xgbGBDT\"]\n",
    "cv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\n",
    "                       \"Algorithm\":ag})\n",
    "\n",
    "g = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\n",
    "g.set_xlabel(\"Mean Accuracy\")\n",
    "g = g.set_title(\"Cross validation scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR 0.8730793894\n",
      "SVC 0.867448927477\n",
      "KNN 0.850682953127\n",
      "decision_tree 0.865188968335\n",
      "random_forest 0.863168198842\n",
      "GBDT 0.884340880717\n",
      "xgbGBDT 0.885426739303\n"
     ]
    }
   ],
   "source": [
    "for i in range(7):\n",
    "    print(ag[i],cv_means[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "class Bagging(object):\n",
    "    \n",
    "    def __init__(self,estimators):\n",
    "        self.estimator_names = []\n",
    "        self.estimators = []\n",
    "        for i in estimators:\n",
    "            self.estimator_names.append(i[0])\n",
    "            self.estimators.append(i[1])\n",
    "        self.clf = LogisticRegression()\n",
    "    \n",
    "    def fit(self, train_x, train_y):\n",
    "        for i in self.estimators:\n",
    "            i.fit(train_x,train_y)\n",
    "        x = np.array([i.predict(train_x) for i in self.estimators]).T\n",
    "        y = train_y\n",
    "        self.clf.fit(x, y)\n",
    "    \n",
    "    def predict(self,x):\n",
    "        x = np.array([i.predict(x) for i in self.estimators]).T\n",
    "        #print(x)\n",
    "        return self.clf.predict(x)\n",
    "        \n",
    "    \n",
    "    def score(self,x,y):\n",
    "        s = precision_score(y,self.predict(x))\n",
    "        #print(s)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "rf = RandomForestClassifier(n_estimators=300,min_samples_leaf=4,class_weight={0:0.745,1:0.255})\n",
    "gbdt = GradientBoostingClassifier(n_estimators=500,learning_rate=0.03,max_depth=3)\n",
    "xgbGBDT = XGBClassifier(max_depth=3, n_estimators=500, learning_rate=0.03)\n",
    "clfs = [logreg, svc, knn, decision_tree, random_forest, gbdt, xgb]\n",
    "\n",
    "bag = Bagging([('xgb',xgb),('lr',lr),('gbdt',gbdt),('rf',rf)])\n",
    "\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.50500000000001"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = 0\n",
    "for i in range(0,10):\n",
    "    num_test = 0.20\n",
    "    X_train, X_cv, Y_train, Y_cv = train_test_split(X_all.values, Y_all.values, test_size=num_test)\n",
    "    bag.fit(X_train, Y_train)\n",
    "    #Y_test = bag.predict(X_test)\n",
    "    acc_xgb = round(bag.score(X_cv, Y_cv) * 100, 2)\n",
    "    score+=acc_xgb\n",
    "score/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag.fit(X_all.values, Y_all.values)\n",
    "Y_test = bag.predict(X_test.values).astype(int)\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": test_df[\"PassengerId\"],\n",
    "        \"Survived\": Y_test\n",
    "    })\n",
    "submission.to_csv(r'submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
